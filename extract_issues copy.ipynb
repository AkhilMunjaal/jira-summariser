{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from jira import JIRA\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "JIRA_API_TOKEN = os.getenv('PERSONAL_JIRA_TOKEN')\n",
    "USERNAME = os.getenv('JIRA_USERNAME')\n",
    "OPENAI_KEY = os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the API key\n",
    "openai.api_key = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login success.\n"
     ]
    }
   ],
   "source": [
    "jira_url = \"https://munjal97akhil.atlassian.net/\"\n",
    "jira = JIRA(server=jira_url,basic_auth=(USERNAME,JIRA_API_TOKEN))\n",
    "if jira:\n",
    "    print('Login success.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<JIRA Board: name='Mavericks BLR', id=1>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the id for your board, name parameter can be used to filter results to boards that match or partially match the specified name.\n",
    "# since i have just 1 board no need to filter.\n",
    "jira.boards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<JIRA Sprint: name='SCRUM Sprint 1', id=1>]\n"
     ]
    }
   ],
   "source": [
    "# To get the issues for a specific or active sprint\n",
    "sprints = jira.sprints(board_id=1)\n",
    "print(sprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<JIRA Sprint: name='SCRUM Sprint 1', id=1>]\n"
     ]
    }
   ],
   "source": [
    "active_sprint = [sprint for sprint in sprints if sprint.state == 'active']\n",
    "print(active_sprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all issues in the active sprint\n",
    "jql = f'sprint = {active_sprint[0].id}'\n",
    "issues = jira.search_issues(jql)\n",
    "\n",
    "all_baklogs = {}\n",
    "for issue in issues:\n",
    "    comments_list = []\n",
    "    comments = issue.fields.comment.comments\n",
    "    if comments:\n",
    "        for comment in comments:\n",
    "            comments_list.append(comment.body)\n",
    "    all_baklogs[issue.key] = {\n",
    "        \"Title\" : issue.fields.summary,\n",
    "        \"Description\" : issue.fields.description,\n",
    "        \"Comments\" : comments_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SCRUM-2': {'Title': 'summarize jira backlogs via llm',\n",
       "  'Description': 'summarize all sprint backlogs using genai',\n",
       "  'Comments': ['once i get all the comments figured wrt each backlog, lets connect the llm to summarize content',\n",
       "   'weâ€™ll use llama3 with ollama ']},\n",
       " 'SCRUM-1': {'Title': 'Testing Jira API',\n",
       "  'Description': 'Testing trial version of jira.',\n",
       "  'Comments': ['authentication successfully done',\n",
       "   'got the board id from project',\n",
       "   'got the active sprint from board ',\n",
       "   'now getting all the backlogs with descriptions and comments.']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_baklogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_model =' gpt-3.5-turbo-16k-0613'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"You're an excellent summariser, who can look at jira backlogs and give summary based on title, description and comments.\n",
    "Do not include any other detail like description, comments when you present the summary. Make sure to use all the comments from the given backlog.\n",
    "While you summarise the comments use the phrase 'Progress so far includes'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_msg = f\"\"\"\n",
    "Jira backlogs with comments:\n",
    "{all_baklogs}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': system_msg\n",
    "    }, {\n",
    "        'role': 'user',\n",
    "        'content': user_msg}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a completion\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-16k-0613\",\n",
    "    messages=messages,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get('choices')[0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, track the cost for summarization effort :\n",
    "\n",
    "# Source: https://openai.com/pricing\n",
    "# Prices in $ per 1000 tokens\n",
    "\n",
    "\n",
    "PRICES = {\n",
    "    \"gpt-4-0613\": {\"input\": 0.03, \"output\": 0.06},\n",
    "    \"gpt-3.5-turbo-0613\": {\"input\": 0.0015, \"output\": 0.002},\n",
    "    \"gpt-4-0125-preview\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4-turbo-preview\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4-1106-preview\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4-1106-vision-preview\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4-turbo-2024-04-09\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "    \"gpt-4-32k\": {\"input\": 0.06, \"output\": 0.12},\n",
    "    \"gpt-3.5-turbo-0125\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "    \"gpt-3.5-turbo-1106\": {\"input\": 0.001, \"output\": 0.002},\n",
    "    \"gpt-3.5-turbo-instruct\": {\"input\": 0.0015, \"output\": 0.002},\n",
    "    \"gpt-3.5-turbo-16k-0613\": {\"input\": 0.003, \"output\": 0.004},\n",
    "    \"whisper-1\": {\"input\": 0.006, \"output\": 0.006},\n",
    "    \"tts-1\": {\"input\": 0.015, \"output\": 0.015},\n",
    "    \"tts-hd-1\": {\"input\": 0.03, \"output\": 0.03},\n",
    "    \"text-embedding-ada-002-v2\": {\"input\": 0.0001, \"output\": 0.0001},\n",
    "    \"text-davinci-003\": {\"input\": 0.02, \"output\": 0.02},\n",
    "    \"text-ada-001\": {\"input\": 0.0004, \"output\": 0.0004},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = response.get('usage')['prompt_tokens']\n",
    "output_tokens = response.data.get('usage')['completion_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cost = input_tokens * PRICES[used_model]['input'] / 1000\n",
    "output_cost = output_tokens * PRICES[used_model]['output'] / 1000\n",
    "cost = input_cost + output_cost\n",
    "print('Cost in $ : ', cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
